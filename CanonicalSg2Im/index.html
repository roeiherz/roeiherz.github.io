<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<style type="text/css">
    table {
        margin-bottom: 5px;
        margin-top: 5px;
    }
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 55%;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img {
        width: 100%
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link, a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0, 0, 0, 0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }


    #authors td {
        padding-bottom: 5px;
        padding-top: 30px;
    }
</style>


<head>

    <title>Canonical Sg2Im</title>
    <meta property="og:title" content="Compositional Video Synthesis with Action Graphs"/>
</head>

<body>
<br>
<center>
    <span style="font-size:36px">Learning Canonical Representations for Scene Graph to Image Generation</span>
    <br><br>

    <span style="font-size:24px">ECCV 2020</span>
    <br><br><br>
    <table align=center>
        <tr>
            <span style="font-size:24px"><a href="https://roeiherz.github.io/">Roei Herzig*</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://www.amirbar.net/">Amir Bar*</a><sup>4</sup></span>
        </tr><br>
        <tr>
            <span style="font-size:24px"><a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://chechiklab.biu.ac.il/">Gal Chechik</a><sup>3</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson</a><sup>1</sup></span> &nbsp;
        </tr>
    </table>

    <span style="font-size:18px"><sup>*</sup>Equally contributed</span>
    <br><br>

    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        <sup>1</sup>Tel-Aviv University
                        <br>
                        <sup>2</sup>UC Berkeley
                        <br>
                        <sup>3</sup>Bar-Ilan University, NVIDIA Research
                        <br>
                        <sup>4</sup>Zebra Medical Vision
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>


<!--    <span style="font-size:24px">Preprint. Under review.</span>-->

    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:24px"><a href="https://arxiv.org/abs/1912.07414"> [Paper]</a></span>
                </center>
            </td>
<!--            <td align=center>-->
<!--                <center>-->
<!--                        <span style="font-size:24px"><a href='https://www.dropbox.com/s/yh110evfjw3bwkf/ActionGraph.pdf?dl=0'> [Slides]</a></span>-->
<!--                </center>-->
<!--            </td>-->

            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='https://github.com/roeiherz/CanonicalSg2Im'> [GitHub]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='data/ECCV2020CanonicalSg2Im.txt'> [Bibtex]</a></span>
                </center>
            </td>
        </tr>
    </table>

</center>
<!--<left>    <span style="font-size:18px"><sup>*</sup>Equally contributed</span> </left>-->

<br><br><br>
<hr>
<table align=center>
    <center><h1>Abstract</h1></center>
</table>

Generating realistic images of complex visual scenes becomes challenging when one wishes to control the structure of the generated images.
Previous approaches showed that scenes with few entities could be controlled using scene graphs, but this approach struggles as the complexity of the graph (the number of objects and edges) increases.
In this work, we show the limitations of current methods are their inability to capture semantic equivalence in graphs, and robustness to graph size and adversarial attacks. We present a novel model to address these by employing a canonical graph representation, which ensures our model robustness to those.
We show improved performance of the model on three different benchmarks: Visual Genome, COCO, and CLEVR.

<br><br><br>
<center><img src="data/figures/teaser.png" align="middle"></center>
<br>
<hr>

<!--        Contribution Section-->
<table align=center>
    <center><h1>Contributions</h1></center>
</table>
Our contributions are thus: <br>
1) We propose a model that uses canonical representations of SGs, thus obtaining <b>stronger invariance properties</b>, and <b>robustness to graph size and adversarial attacks </b> than existing methods.
<br>
2) We show how to learn the canonicalization process from data.
<br>
3) We use our canonical representations within an SG-to-image model and demonstrate our approach results in an improved generation on Visual Genome, COCO, and CLEVR, compared to the state-of-the-art baselines.
<br><br>
<!--<center><img src="data/videos/contributions.gif" width=789 align="middle"></center>-->
<!--<br>-->
<hr>
<br>


<!--        Canonicalization Section-->
<table align=center>
    <center><h1 id="canonicalization">Scene Graph Canonicalization</h1></center>
</table>

<p>
    We propose an approach to canonicalize graphs to enforce invariance to these equivalences since multiple logically equivalent SGs can represent the same image.
    Given an input of SG, our model calculated a weighted canonicalized graph, followed a GCN for generating robust layouts.

</p>
<center><img src="data/figures/architecture.png" align="middle"></center>
<br><br>
<p>
    Given the input graph, two weighted completion steps are performed.
    The first step is the Converse Completion: Anti-symmetry relations are added with its corresponding converse weight (e.g., R(left, right)).
    The second step is the Transitive Completion: Transitive relations are added, and we set the weight to be the product of weights.
    Finally, the model learns to form the canonical graph.
    <br>
    Here an example of the process.

</p>
<center><img src="data/videos/Canonicalization.gif" align="middle"></center>


<!--        Model Section-->
<table align=center>
    <center><h1 id="model">WSGC Model</h1></center>
</table>
<MATH>

    We proposed the <b>WSGC</b> <b>model</b> (Weighted Scene Graph Canonicalization) that learns the canonicalization process from data.
    However, a method for obtaining a weighted relation graph is computationally demanding (though poly-time)
    for large SGs since the transitivity step is performed on a dense graph (most weights will be non-zero). Thus, we present a faster alternative.
    We propose replacing the converse completion step with a sampling-based approach using the REINFORCE algorithm that samples completed edges, but always gives them a weight of 1 when added.
    In this way, the transitive step is computed on a much sparser graph with weights 1.




</MATH>
<br><br>
<center><img src="data/figures/wsgcs.jpg" width=789 align="middle"></center>
<br><br>
<hr>
<br>

<!--        Results Section-->
<table align=center width=850px>
    <center><h1>Results</h1></center>
</table>

<table align=center width=850px>
    <center><h1>Generation with large graph sizes</h1></center>
</table>
<table>
    <tr>
        <p> We show our model generalizes over the number of objects in the graph.</p>
        <center><img src="data/figures/large_graph_size.png"/></center>
        <p> For example, the baseline and our model were trained on a fixed number of objects,
            but when tested on a large number, the baselines collapse (top row vs. down row). </p>
    </tr>
</table>
<br>

<table align=center width=850px>
    <center><h1>Robustness to semantic equivalent</h1></center>
</table>
<table>
    <tr>
        <p>A key advantage of our model is that it produces similar layouts for semantically equivalent graphs.
            To achieve this, we generate a semantically equivalent SG by randomly choosing to include/exclude edges
            that don't change the semantics. </p>
        <center><img src="data/figures/semantic_eqv.png"/></center>
        <p>Each input SG is modified and mapped into a semantically equivalent random SG at test time.
            The layout-to-image model is fixed to LostGAN and different SG-to-layout models are tested.
            (a) Original SG (partial) (b) A semantically equivalent SG (partial) (c) WSGC SG-to-layout for the equivalent SG.
            (d) Sg2Im SG-to-layout for the equivalent SG.</p>
    </tr>
</table>
<br>

<table align=center width=850px>
    <center><h1>Robustness to adversarial modifications and noise</h1></center>
</table>

<table>
    <tr>
        <p> Conflicting edges in the input SG could be due to unintended or adversarial modifications.
            We analyze how modifications affect the model by randomly modifying 10% of the relations.
        </p>
        <center> <img src="data/figures/table2.png"/></center>
        <p> We evaluate the robustness of the learned Canonical Representation for models which were trained on Packed COCO (images with 16+ objects in COCO).
            We compare our WSGC model to the Sg2Im baseline with a GCN of 5 layers and even more.
            It can be seen that increasing the GCN size comes at the price of overfitting.
<!--            For each SG, a semantically equivalent SG is sampled and evaluated at test time.-->
<!--            Additionally, models are evaluated on Noisy SGs, for which edges contain 10% randomly chosen relations.-->
        </p>
    </tr>
</table>
<br>

<hr>
<table align=center width=850px>
    <center><h1>Attribute SPADE</h1></center>
</table>
<p> The key idea in the AttSPADE model is to condition generation on the attributes,
    as opposed to only the object class as done in current models.</p>

<table align=center width=850px>
    <center><h1>Scene manipulation</h1></center>
</table>
<table>
    <tr>
        <center>
            <img src="data/videos/scene_manipulation.gif"/></center>
        <p> A demonstration of the AttSPADE that by manipulating the attributes of the objects,
            the scene is generated accordingly. </p>
    </tr>
</table>
<br>


<table align=center width=850px>
    <center><h1>Qualitative examples</h1></center>
</table>
<table>
    <tr>
        <center> <img src="data/figures/coco128_generation_compare_eccv.jpg"/></center>
        <p> Selected GT layout-to-image generation results on COCO-Stuff dataset on 128x128 resultion.
            Here, the GT layout also includes masks. (a) GT image. (b) Generation with LostGAN model.
            (c) Generation with Grid2Im. (d) Generation with AttSPADE model (ours). </p>
    </tr>
</table>
<br>

<table>
    <tr>
        <center> <img src="data/figures/coco256_generation_compare_eccv.jpg"/></center>
        <p> Selected generation results on the COCO-Stuff dataset at 256x256 resolution.
            Here the GT layout also includes masks. (a) GT image. (b) Generation with Grid2Im using the GT layout.
            (c) Generation with Grid2Im No-att from the scene graph (GT layout not used).
            (d) Generation with AttSPADE model (Ours) using the GT layout.
            (e) Generation with WSGC + AttSPADE model (Ours) from the scene graph (GT layout not used) </p>
    </tr>
</table>
<br>

<table>
    <tr>
        <center> <img src="data/figures/vg128_generation_compare_eccv.jpg"/></center>
        <p> Selected scene-graph-to-image results on Visual Genome dataset on 128x128 resolution.
            Here, the GT layout includes only boxes. (a) GT image. (b) Generation using LostGAN using the GT layout.
            (c) Generation with the WSGC + LostGAN using the scene graph (GT layout not used).
            (d) Generation with the AttSPADE model (Ours) using the GT Layout.
            (e) Generation with the WSGC + AttSPADE model (Ours) using the scene graph (GT layout not used). </p>
    </tr>
</table>
<br>

<table>
    <tr>
        <center> <img src="data/figures/vg256_generation_compare_eccv.jpg"/></center>
        <p> Selected scene-graph-to-image results on the Visual Genome dataset at 256x256 resolution.
            Here, the GT layout includes only boxes. (a) GT image. (b) Generation with the AttSPADE model (Ours) using GT Layout.
            (c) Generation with the WSGC + AttSPADE model (Ours) using the scene graph (GT layout not used). </p>
    </tr>
</table>
<br>


<table align=center width=850px>
    <center><h1>Quantitative results</h1></center>
</table>
<table>
    <tr>
        <center> <img src="data/figures/attspade_table.png"/></center>
        <p> Quantitative comparisons for SG-to-image methods using Inception Score (higher is better),
            FID (lower is better) and Diversity Score (higher is better).
            Evaluation is done on the COCO-Stuff and VG datasets. </p>
    </tr>
</table>
<br>

<br>
<hr>
<br>

<table align=center>
    <center><h1>Paper</h1></center>
    <tr>
        <td><a href="https://arxiv.org/pdf/1912.07414.pdf">
            <img class="layered-paper-big" style="height:175px; width: 150px; margin-bottom: 50px" src="data/figures/paper.png"/></a></td>
        <td><a href="https://arxiv.org/pdf/1912.07414.pdf"></a></td>
        <td>
            <span style="font-size:14pt"> Roei Herzig*, Amir Bar*, Huijuan Xu, Gal Chechik, Trevor Darrell, Amir Globerson
                <br><br>
                <i>Learning Canonical Representations for Scene Graph to Image Generation</i>
                <br><br>
                In ECCV, 2020
                <br>
                Hosted on <a href="https://arxiv.org/abs/1912.07414">arXiv</a>
            </span>
            <br><br>
            <span style="font-size:14px"><sup>*</sup>Equal contribution</span>
        </td>
        <span style="font-size:4pt"><a href="https://arxiv.org/pdf/1912.07414.pdf"><br></a></span>
    </tr>
</table>

<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <left>
                <center><h1>Acknowledgements</h1></center>

                This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research
                and innovation programme (grant ERC HOLI 819080). Prof. Darrell's group was supported in part by DoD, NSF, BAIR, and BDD.
                We would also like to thank Xiaolong Wang for comments on drafts.
            </left>
        </td>
    </tr>
</table>


</body>
</html>
