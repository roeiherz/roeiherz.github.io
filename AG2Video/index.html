<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<!--<script type="text/javascript">google.load("jquery", "1.3.2");</script>-->
<style type="text/css">
    table {
        margin-bottom: 5px;
        margin-top: 5px;
    }
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 60%;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img {
        width: 100%
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link, a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0, 0, 0, 0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }


    #authors td {
        padding-bottom: 5px;
        padding-top: 30px;
    }
</style>


<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108048997-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108048997-1');
  </script>

    <title>AG2Video</title>
    <meta property="og:title" content="Compositional Video Synthesis with Action Graphs"/>
</head>

<body>
<br>
<center>
    <span style="font-size:36px">Compositional Video Synthesis with Action Graphs</span>
    <br><br>

    <span style="font-size:24px">ICML 2021</span>
    <br><br><br>
    <table align=center>
        <tr>
            <span style="font-size:24px"><a href="http://www.amirbar.net/">Amir Bar*</a><sup>1</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://roeiherz.github.io/">Roei Herzig*</a><sup>1</sup></span>
        </tr><br>
        <tr>
            <span style="font-size:24px"><a href="http://www.cs.cmu.edu/~xiaolonw/">Xiaolong Wang</a><sup>2,3</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://anna-rohrbach.net/">Anna Rohrbach</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://chechiklab.biu.ac.il/">Gal Chechik</a><sup>4</sup></span> &nbsp;
            <span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>2</sup></span> &nbsp;
            <span style="font-size:24px"><a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson</a><sup>1</sup></span> &nbsp;
        </tr>
    </table>

    <span style="font-size:18px"><sup>*</sup>Equally contributed</span>
    <br><br>

    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        <sup>1</sup>Tel-Aviv University &nbsp;<sup>2</sup>UC Berkeley &nbsp;<sup>3</sup>UC San Diego &nbsp;<sup>4</sup>Bar-Ilan University, NVIDIA Research
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>


<!--    <span style="font-size:24px">Preprint. Under review.</span>-->

    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:24px"><a href="https://arxiv.org/abs/2006.15327"> [Paper]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='https://github.com/roeiherz/AG2Video'> [GitHub]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:24px"><a href='data/2020ActionGraphs.txt'> [Bibtex]</a></span>
                </center>
            </td>
        </tr>
    </table>
<br>
<center><img src="data/videos/ag2vid_task.gif" align="middle"></center>

</center>
<!--<left>    <span style="font-size:18px"><sup>*</sup>Equally contributed</span> </left>-->

<br><br><br>
<hr>
<table align=center>
    <center><h1>Goals</h1></center>
</table>
Our main goal is to learn to synthesize videos of actions. More specifically, our model should be able to synthesize videos given multiple (potentially simulatnious) timed actions with multiple number of objects. Towards this end, we present the <a href="#action_graphs">Action Graph</a> (AG) structure which we use to represent timed actions and objects, and the <a href="#model">AG2Vid</a> model which can synthesize videos conditioned on actions given in AGs and initial image and layout.
<br><br><br>
<hr>
<br>
<table align=center>
    <center><h1 id="action_graphs">Action Graphs</h1></center>
</table>
<p>
    Classic work in cognitive-psychology argues that actions (and more generally events) are bounded regions of space-time and are composed of atomic action units (Quin, 1985; Zacks and Tversky, 2001). In a video, multiple actions can be applied to one or more objects, changing the relationships between the object and the subject of an action over time. Based on these observations, we introduce a formalism we call an <b>Action Graph</b>, a graph where nodes are objects, and the edges are actions specified by their start and end time.
</p>
<center><img src="data/videos/ag_vd.gif" align="middle"></center>
<p>
    <b>Action Graph</b> is a natural and convenient structure representing the dynamics of actions between objects over time.


</p>
<!--        , and propose a new video synthesis task from action graphs. We present a novel action-graph-to-video (AG2Vid) model for this task-->
<!--and show we can synthesize goal-oriented videos on the CATER and Something Something datasets.-->
<br>
<br><br>
<hr>
<br>


<!--        Model Section-->
<table align=center>
    <center><h1 id="model">Model</h1></center>
</table>
<MATH>
We proposed the AG2Vid model. The video synthesis process is done in a coarse-to-fine manner, starting from scheduling the time-specific actions of the Action Graph. This is followed by the prediction of the next object locations which are then used to synthesize the next image pixels v<sub>t</sub>.
To execute an Action Graph, each action (and its corresponding edge) is assigned with a "clock" which marks the progress of the action in a particular timestep. The Action Graph A<sub>t</sub> at time t describes the execution stage of each action at time t.
<br>
<center><img src="data/figures/clocked_edges.png" width=789 align="middle"></center>
<br>

Using the Action Graph at time t A<sub>t</sub> and together with the previous layout and frame (l<sub>t-1</sub>, v<sub>t-1</sub>) the AG2Vid model synthesizes the next frame v<sub>t</sub>.
</MATH>
<br><br>
<center><img src="data/figures/model.png" width=789 align="middle"></center>
<br><br>
<hr>
<br>

<!--        Results Section-->
<table align=center width=850px>
    <center><h1>Results</h1></center>
</table>
<table align=center width=850px>
    <center><h1>Qualitative examples</h1></center>
</table>
<table>
    <tr>
        <center>
            <img src="data/videos/standard_actions.gif"/></center>
        <p>Qualitative examples for the generation of actions on the CATER and Something-Something V2 datasets.
            We use the AG2Vid model to generate videos of multiple (potentially simultanious) actions on CATER and single actions on Something-Something V2.</p>
    </tr>
</table>
<br> <br>

<table align=center width=850px>
    <center><h1 id="sequential">Zero-shot synthesis of sequential actions</h1></center>
</table>
<p>We use the model trained on single actions on the Something-Something V2 dataset, and define more complex actions graphs, which contain sequential actions. In the given example, we define the sequence "Move up", "Move down", and "Move right".</p>
<table>
    <tr>
        <center>
            <img src="data/videos/seque.gif"/></center>
    </tr>
</table>
<br>
<table align=center width=850px>
    <center><h1 id="simultaneous">Zero-shot synthesis of simultaneous actions</h1></center>
</table>
<p>We use the model trained on single actions on the Something-Something V2 dataset, and define more complex actions graphs, which contain simultaneous actions. In the given example, we define the actions "Move right", "Move left" simultaneously.</p>
<table>
    <tr>
        <center>
            <img src="data/videos/simu.gif"/></center>
    </tr>
</table>
<br>

<table align=center width=850px>
    <center><h1 id="new_actions">Zero-shot synthesis of new action composites</h1></center>
</table>

<table>
    <tr>
                <p>Qualitative examples for the generation of new action composites on the Something-Something V2 and CATER datasets.
            We use the Action Graph representation to define 4 new action composites out of the existing actions, and use our AG2Vid model to generate corresponding videos.</p>

    </tr>
    <tr>
        <center>
            <img src="data/videos/compositional_results.gif"/></center>
    </tr>
</table>
<br>

<br><br>
<hr>
<br>

<table align=center>
    <center><h1>Paper</h1></center>
    <tr>
        <td><a href="https://arxiv.org/pdf/2006.15327.pdf"><img class="layered-paper-big" style="height:175px; width: 150px; margin-bottom: 50px"
                                                                src="data/figures/paper.png"/></a></td>
        <td><a href="https://arxiv.org/pdf/2006.15327.pdf"></a></td>
        <td>
                <span style="font-size:14pt"> Amir Bar*, Roei Herzig*, Xiaolong Wang, Anna Rohrbach, Gal Chechik, Trevor Darrell, Amir Globerson<br>
              <i>Compositional Video Synthesis with Action Graphs</i><br>
            ICML 2021<br>
            Hosted on <a href="https://arxiv.org/abs/2006.15327">arXiv</a>
                </span>
            <br>
            <span style="font-size:18px"><sup>*</sup>Equal contribution</span>
        </td>
        <span style="font-size:4pt"><a href="https://arxiv.org/pdf/2006.15327.pdf"><br></a></span>
    </tr>
</table>
<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <center><h1>Related Works</h1></center>
            Our work builds and borrows code from multiple past works such as <a href="https://github.com/google/sg2im">SG2Im</a>, <a href="https://roeiherz.github.io/CanonicalSg2Im/">Canonical-SG2IM</a> and <a href="https://github.com/NVIDIA/vid2vid">Vid2Vid</a>. If you found our work helpful, consider citing these works as well.
        </td>
    </tr>
</table>

<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <left>
                <center><h1>Acknowledgements</h1></center>
            We would like to thank Lior Bracha for her help running MTurk experiments, and to Haggai Maron and Yuval Atzmon for helpful feedback and discussions. This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Trevor Darrellâ€™s group was supported in part by DoD including DARPA's XAI, LwLL, and/or SemaFor programs, as well as BAIR's industrial alliance programs. Gal Chechik's group was supported by the Israel Science Foundation (ISF 737/2018), and by an equipment grant to Gal Chechik and Bar-Ilan University from the Israel Science Foundation (ISF 2332/18). This work was completed in partial fulfillment for the Ph.D degree of Amir Bar.
            </left>
        </td>
    </tr>
</table>


</body>
</html>
