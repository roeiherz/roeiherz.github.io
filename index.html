<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Roei Herzig</title>
  
  <meta name="author" content="Roei Herzig">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="4VZM_QHD0ktSdSTOi7khsldecOhYVsbJ7TPQkx5lvUc" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roei Herzig</name>
              </p>
              <p>
                Hi there! I'm Roei, a first-year Ph.D. student in Computer Science at <a href="https://www.tau.ac.il/">Tel Aviv University</a>,
                working with <a href="https://research.nvidia.com/person/gal-chechik">Prof. Gal Chechik</a>,
                <a href="http://www.cs.tau.ac.il/~gamir/">Prof. Amir Globerson </a>
                and <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>,
                and a member of <a href="https://bair.berkeley.edu/">the Berkeley AI Research Lab</a>.
              </p>
              <p>
                I'm also a Machine Learning & Deep Learning Researcher, I have worked at <a href="https://www.getnexar.com/"> Nexar </a> and
                <a href="https://traxretail.com/"> Trax Image Recognition </a> in the last 5 years.
                I graduated <i> Magna Cum Laude</i> from <a href="https://www.tau.ac.il/">Tel Aviv University</a>
                with MSc (CS), BSc (CS) and BSc (Physics).
              </p>
              <p>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                I'm looking for a strong MSc students that wish to collaborate and publish in top-tier conferences on Geometry and Learning for 3D and Video.
              </p>
              <p style="text-align:center">
                <a href="mailto:roeiherz@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/roeiherzig">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/roeiherz">Github</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp-->
                <a href="https://www.linkedin.com/in/roei-herzig-7534615a/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://scholar.google.co.il/citations?user=6Q-289IAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/RoeiHerzig.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/RoeiHerzig_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I mainly focus on machine learning models and deep learning methods for <b>video and image understanding using structure</b>
                (e.g. Structured Prediction). I believe our world is compositional and humans don't perceive the world as raw pixels.
                Moreover, structured models can enjoy the properties of generalization and inductive-bias, which I find critical, especially at the intersections of vision, language and robotics.
              </p>
              <p>
                Research Interest: <br>
                <ul>
                  <li>Vision & Language: Object Detection, Scene Understanding, Visual Reasoning. </li>
                  <li>Vision & Robotics: Transfer Learning, Structured Representation, Semantic Understanding. </li>
                  <li>Machine Learning & Deep Learning: Semi-Supervised Learning, Self-Supervised Learning, Unsupervised Learning, Generative Models, Graph Neural Networks. </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/canonical_gen.png" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.07414">
                <papertitle>Learning Canonical Representations for Scene Graph to Image Generation</papertitle>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              </a>
              <br>
              <strong>Roei Herzig*</strong>,
              <a href="http://www.amirbar.net/">Amir Bar*</a>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
<!--              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020-->
              <em> ArXiv preprint </em>, 2019
              <br>
<!--              <a href="https://github.com/shikorab/SceneGraph">Code</a> /-->
              <a href="data/herzig2019canonical.bib">bibtex</a>
              <br>
              <p></p>
              <p>

                We present a novel model which can inherently learn canonical graph representations and
                can better capture object representation independently of the number of objects in the graph,
                thus generating images of complex visual scenes and ensuring that semantically similar scene graphs will result in similar predictions.
                We show improved performance of the model on three different benchmarks: Visual Genome, COCO and CLEVR.

              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/compositional.png" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.09930">
                <papertitle>Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</papertitle>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              </a>
              <br>
              Joanna Materzynska,
              <a href="http://tetexiao.com/">Tete Xiao</a>,
              <strong>Roei Herzig</strong>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang*</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a>
              <br>
<!--              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020-->
              <em> ArXiv preprint </em>, 2019
              <br>
<!--              <a href="https://github.com/shikorab/SceneGraph">Code</a> /-->
              <a href="data/herzig2019compositional.bib">bibtex</a>
              <br>
              <p></p>
              <p>

                We propose a novel compositional action recognition task where the training combinations of verbs and nouns do not overlap with the test set.
                We show the effectiveness of our approach on the proposed compositional task and
                a few-shot compositional setting which requires the model to generalize across both object appearance and action category.

              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dsg.jpg" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.10200">
                <papertitle>Differentiable Scene Graphs</papertitle>
<!--                <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
              </a>
              <br>
              Moshiko Raboh* ,
              <strong>Roei Herzig*</strong>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020
<!--              <em> arXiv preprint </em> , 2019-->
              <br>
<!--              <a href="https://github.com/shikorab/SceneGraph">Code</a> /-->
              <a href="data/herzig2019DSG.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose an intermediate “graph-like” representation (<i>DSGs</i>) that can be learned in an end-to-end manner
                from the supervision for a downstream visual reasoning task, which achieves a new state-of-the-art results
                on <i>Referring Relationships</i> task.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scene_understanding_resize.gif" alt="fast-texture" width="200" height="160", loop=infinite>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.01233">
                <papertitle>Spatio-Temporal Action Graph Networks</papertitle>
<!--                <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Elad Levi* ,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://people.eecs.berkeley.edu/~hangg/">Hang Gao</a>,
              Eli Brosh,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
              <em> IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) </em>, 2019&nbsp <font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/roeiherz/STAG-Nets">Code</a> /
              <a href="data/herzig2019STAG.bib">bibtex</a>
              <br>
              <p></p>
              <p> We propose a <i>latent</i> inter-object graph representation for activity recognition
                that explores the visual interaction between the objects in a self-supervised manner. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/localizationNexar.jpg" alt="fast-texture" width="200" height="160", loop=infinite>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1905.03706">
                <papertitle>Accurate Visual Localization for Automotive Applications</papertitle>
              </a>
              <br> Eli Brosh*, Matan Friedmann*, Ilan Kadar*, Lev Yitzhak Lavy*, Elad Levi*, Shmuel Rippa*,
              Yair Lempert, Bruno Fernandez-Ruiz, <strong>Roei Herzig</strong>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
              <em> Workshop on Autonomous Driving at CVPR </em> , 2019
              <br>
              <a href="https://blog.getnexar.com/announcing-ai-powered-image-retrieval-method-for-better-localization-in-cities-6fc482b4b0f">blog</a> /
              <a href="https://github.com/getnexar/Nexar-Visual-Localization">code</a> /
              <a href="https://github.com/getnexar/Nexar-Visual-Localization#dataset--benchmark">dataset</a> /
              <a href="data/herzig2019localization.bib">bibtex</a>
              <br>
              <p></p>
              <p> We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues with on
                a new large-scale driving dataset based on video and GPS data.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/densely_packed_scenes_store.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Goldman_Precise_Detection_in_Densely_Packed_Scenes_CVPR_2019_paper.html">
                <papertitle>Precise Detection in Densely Packed Scenes</papertitle>
              </a>
              <br>
              Eran Goldman*,
              <strong>Roei Herzig*</strong>, Aviv Eisenschtat* ,
              <a href="http://www.eng.biu.ac.il/goldbej/">Jacob Goldberger</a>,
              <a href="https://talhassner.github.io/home/">Tal Hassner</a>
              <br>
              <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2019
              <br>
              <a href="https://github.com/eg4000/SKU110K_CVPR19">code</a> /
              <a href="https://github.com/eg4000/SKU110K_CVPR19#dataset">dataset</a> /
              <a href="data/herzig2019dense.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We collect a new <i>SKU-110K</i> dataset which takes detection challenges to unexplored territories,
                and propose a novel mechanism to learn deep overlap rates for each detection. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mapping_imgs2sg.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction.pdf">
                <papertitle>Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</papertitle>
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Moshiko Raboh* ,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
              <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2018
              <br>
              <a href="https://github.com/shikorab/SceneGraph">code</a> /
              <a href="data/herzig2018img2sg.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose a novel invariant graph network for mapping images to scene graphs using the permutation invariant
                property, which achieves a new state-of-the-art results on Visual Genome dataset. </p>
            </td>
          </tr>
<!--          <tr onmouseout="dpzlearn_stop()" onmouseover="dpzlearn_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='dpzlearn_image'><img src='images/dpzlearn_after.jpg'></div>-->
<!--                <img src='images/dpzlearn_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function dpzlearn_start() {-->
<!--                  document.getElementById('dpzlearn_image').style.opacity = "1";-->
<!--                }-->

<!--                function dpzlearn_stop() {-->
<!--                  document.getElementById('dpzlearn_image').style.opacity = "0";-->
<!--                }-->
<!--                dpzlearn_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/1904.05822">-->
<!--                <papertitle>Learning Single Camera Depth Estimation using Dual-Pixels</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,-->
<!--              <a href="http://nealwadhwa.com">Neal Wadhwa</a>, Sameer Ansari,-->
<!--              <strong>Jonathan T. Barron</strong>-->
<!--              <br> arXiv:1904.05822-->
<!--              <br>-->
<!--              <p></p>-->
<!--              <p>Considering the optics of dual-pixel image sensors improves monocular depth estimation techniques.</p>-->
<!--            </td>-->
<!--          </tr>-->
<!--          -->
<!--          <tr onmouseout="loss_stop()" onmouseover="loss_start()" bgcolor="#ffffd0">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='loss_image'><img src='images/loss_after.png'></div>-->
<!--                <img src='images/loss_before.png'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function loss_start() {-->
<!--                  document.getElementById('loss_image').style.opacity = "1";-->
<!--                }-->

<!--                function loss_stop() {-->
<!--                  document.getElementById('loss_image').style.opacity = "0";-->
<!--                }-->
<!--                loss_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/open?id=1xpZ0fL9h1y9RfcTyPgVkxUrF3VwdkBvq">-->
<!--                <papertitle>A General and Adaptive Robust Loss Function</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <strong>Jonathan T. Barron</strong>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1701.03077">arxiv</a> /-->
<!--              <a href="https://drive.google.com/open?id=1HNveL7xSNh6Ss7sxLK8Mw2L1Fc-rRhL4">supplement</a> /-->
<!--              <a href="https://youtu.be/BmNKbnF69eY">video</a> /-->
<!--              <a href="https://www.youtube.com/watch?v=4IInDT_S0ow&t=37m22s">talk</a> / -->
<!--              <a href="https://github.com/google-research/google-research/tree/master/robust_loss">tensorflow code</a> /-->
<!--              <a href="https://github.com/jonbarron/robust_loss_pytorch">pytorch code</a> /-->
<!--              <a href="data/BarronCVPR2019_reviews.txt">reviews</a> /-->
<!--              <a href="data/BarronCVPR2019.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>A single robust loss function is a superset of many other common robust loss functions, and allows training to automatically adapt the robustness of its own loss.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="mpi_stop()" onmouseover="mpi_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='mpi_image'><img src='images/mpi_after.jpg'></div>-->
<!--                <img src='images/mpi_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function mpi_start() {-->
<!--                  document.getElementById('mpi_image').style.opacity = "1";-->
<!--                }-->

<!--                function mpi_stop() {-->
<!--                  document.getElementById('mpi_image').style.opacity = "0";-->
<!--                }-->
<!--                mpi_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/file/d/1TU5L6fnt4Kd49IUOU7aNxor5NIgdHuNG/view?usp=sharing">-->
<!--                <papertitle>Pushing the Boundaries of View Extrapolation with Multiplane Images</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://people.eecs.berkeley.edu/~pratul/">Pratul P. Srinivasan</a>, Richard Tucker,-->
<!--              <strong>Jonathan T. Barron</strong>,-->
<!--              <a href="http://cseweb.ucsd.edu/~ravir/">Ravi Ramamoorthi</a>,-->
<!--              <a href="http://graphics.stanford.edu/~renng/">Ren Ng</a>,-->
<!--              <a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Award Finalist)</strong></font>-->
<!--              <br>-->
<!--              <a href="https://drive.google.com/file/d/1GUW_n-BAn9Q4VntEA_OTHNJiHO7XfC62/view?usp=sharing">supplement</a> /-->
<!--              <a href="https://www.youtube.com/watch?v=aJqAaMNL2m4">video</a> /-->
<!--              <a href="data/SrinivasanCVPR2019.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>View extrapolation with multiplane images works better if you reason about disocclusions and disparity sampling frequencies.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="unprocessing_stop()" onmouseover="unprocessing_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='unprocessing_image'><img src='images/unprocessing_after.jpg'></div>-->
<!--                <img src='images/unprocessing_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function unprocessing_start() {-->
<!--                  document.getElementById('unprocessing_image').style.opacity = "1";-->
<!--                }-->

<!--                function unprocessing_stop() {-->
<!--                  document.getElementById('unprocessing_image').style.opacity = "0";-->
<!--                }-->
<!--                unprocessing_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/file/d/1H0Wtd&#45;&#45;un2JN76dUJN8iC9fWfkA16n8D/view?usp=sharing">-->
<!--                <papertitle>Unprocessing Images for Learned Raw Denoising</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://timothybrooks.com/">Tim Brooks</a>,-->
<!--              <a href="https://people.eecs.berkeley.edu/~bmild/">Ben Mildenhall</a>,-->
<!--              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,-->
<!--              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>,-->
<!--              <a href="http://www.dsharlet.com/">Dillon Sharlet</a>,-->
<!--              <strong>Jonathan T. Barron</strong>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1811.11127">arxiv</a> /-->
<!--              <a href="http://timothybrooks.com/tech/unprocessing/">project page</a> /-->
<!--              <a href="https://github.com/google-research/google-research/tree/master/unprocessing">code</a> / -->
<!--              <a href="data/BrooksCVPR2019.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>We can learn a better denoising model by processing and unprocessing images the same way a camera does.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="motionblur_stop()" onmouseover="motionblur_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='motionblur_image'><img src='images/motionblur_after.jpg'></div>-->
<!--                <img src='images/motionblur_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function motionblur_start() {-->
<!--                  document.getElementById('motionblur_image').style.opacity = "1";-->
<!--                }-->

<!--                function motionblur_stop() {-->
<!--                  document.getElementById('motionblur_image').style.opacity = "0";-->
<!--                }-->
<!--                motionblur_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/file/d/1hWpA4f6iLVcOkZI3zEAAWKARSQhnVgbY/view?usp=sharing">-->
<!--                <papertitle>Learning to Synthesize Motion Blur</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://timothybrooks.com/">Tim Brooks</a>,-->
<!--              <strong>Jonathan T. Barron</strong>-->
<!--              <br>-->
<!--              <em>CVPR</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1811.11745">arxiv</a> /-->
<!--              <a href="https://drive.google.com/file/d/1dUQwBMmQdYYIP0zHR_nDQY-uQbaMdcSN/view?usp=sharing">supplement</a> /-->
<!--              <a href="http://timothybrooks.com/tech/motion-blur/">project page</a> /-->
<!--              <a href="https://www.youtube.com/watch?v=8T1jjSz-2V8">video</a> /-->
<!--              <a href="data/BrooksBarronCVPR2019.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>Frame interpolation techniques can be used to train a network that directly synthesizes linear blur kernels.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="darkflash_stop()" onmouseover="darkflash_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='darkflash_image'><img src='images/darkflash_after.png'></div>-->
<!--                <img src='images/darkflash_before.png'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function darkflash_start() {-->
<!--                  document.getElementById('darkflash_image').style.opacity = "1";-->
<!--                }-->

<!--                function darkflash_stop() {-->
<!--                  document.getElementById('darkflash_image').style.opacity = "0";-->
<!--                }-->
<!--                darkflash_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/1901.01370">-->
<!--                <papertitle>Stereoscopic Dark Flash for Low-light Photography</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.andrew.cmu.edu/user/jianwan2/">Jian Wang</a>,-->
<!--              <a href="https://people.csail.mit.edu/tfxue/">Tianfan Xue</a>,-->
<!--              <strong>Jonathan T. Barron</strong>-->
<!--              <a href="http://people.csail.mit.edu/jiawen/">Jiawen Chen</a>-->
<!--              <br>-->
<!--              <em>ICCP</em>, 2019-->
<!--              <br>-->
<!--              <p></p>-->
<!--              <p>-->
<!--                By making one camera in a stereo pair hyperspectral we can multiplex dark flash pairs in space instead of time.-->
<!--              </p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="motionstereo_stop()" onmouseover="motionstereo_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='motionstereo_image'><img src='images/motionstereo_after.png'></div>-->
<!--                <img src='images/motionstereo_before.png'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function motionstereo_start() {-->
<!--                  document.getElementById('motionstereo_image').style.opacity = "1";-->
<!--                }-->

<!--                function motionstereo_stop() {-->
<!--                  document.getElementById('motionstereo_image').style.opacity = "0";-->
<!--                }-->
<!--                motionstereo_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/file/d/1AABFJ3NgD5DAo5JEpEjWZrcQNzjZnvW9/view?usp=sharing">-->
<!--                <papertitle>Depth from Motion for Smartphone AR</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="https://www.linkedin.com/in/valentinjulien/">Julien Valentin</a>,-->
<!--              <a href="https://www.linkedin.com/in/adarshkowdle/">Adarsh Kowdle</a>,-->
<!--              <strong>Jonathan T. Barron</strong>, <a href="http://nealwadhwa.com">Neal Wadhwa</a>, and others-->
<!--              <br>-->
<!--              <em>SIGGRAPH Asia</em>, 2018-->
<!--              <br>-->
<!--              <a href="data/Valentin2018.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>Depth cues from camera motion allow for real-time occlusion effects in augmented reality applications.</p>-->
<!--            </td>-->
<!--          </tr>-->

<!--          <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <div class="one">-->
<!--                <div class="two" id='portrait_image'><img src='images/portrait_after.jpg'></div>-->
<!--                <img src='images/portrait_before.jpg'>-->
<!--              </div>-->
<!--              <script type="text/javascript">-->
<!--                function portrait_start() {-->
<!--                  document.getElementById('portrait_image').style.opacity = "1";-->
<!--                }-->

<!--                function portrait_stop() {-->
<!--                  document.getElementById('portrait_image').style.opacity = "0";-->
<!--                }-->
<!--                portrait_stop()-->
<!--              </script>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://drive.google.com/file/d/13i6DlS9UhGVKmwslLUFnKBwdxFRVQeQj/view?usp=sharing">-->
<!--                <papertitle>Synthetic Depth-of-Field with a Single-Camera Mobile Phone</papertitle>-->
<!--              </a>-->
<!--              <br>-->
<!--              <a href="http://nealwadhwa.com">Neal Wadhwa</a>,-->
<!--              <a href="http://rahuldotgarg.appspot.com/">Rahul Garg</a>,-->
<!--              <a href="http://graphics.stanford.edu/~dejacobs/">David E. Jacobs</a>, Bryan E. Feldman, Nori Kanazawa, Robert Carroll,-->
<!--              <a href="http://www.cs.cmu.edu/~ymovshov/">Yair Movshovitz-Attias</a>,-->
<!--              <strong>Jonathan T. Barron</strong>, Yael Pritch,-->
<!--              <a href="http://graphics.stanford.edu/~levoy/">Marc Levoy</a>-->
<!--              <br>-->
<!--              <em>SIGGRAPH</em>, 2018-->
<!--              <br>-->
<!--              <a href="https://arxiv.org/abs/1806.04171">arxiv</a> /-->
<!--              <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">blog post</a> /-->
<!--              <a href="data/Wadhwa2018.bib">Bibtex</a>-->
<!--              <p></p>-->
<!--              <p>Dual pixel cameras and semantic segmentation algorithms can be used for shallow depth of field effects.</p>-->
<!--              <p>This system is the basis for "Portrait Mode" on the Google Pixel 2 smartphones</p>-->
<!--            </td>-->
<!--          </tr>-->

        </tbody></table>

<!--        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td>-->
<!--              <heading>Service</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <table width="100%" align="center" border="0" cellpadding="20"><tbody>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>-->
<!--            </td>-->
<!--          </tr>-->
<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/cs188.jpg" alt="cs188">-->
<!--            </td>-->
<!--            <td width="75%" valign="center">-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>-->
<!--              <br>-->
<!--              <br>-->
<!--              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>-->
<!--            </td>-->
<!--          </tr>-->
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Webside template credits </a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143788442-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-143788442-1');
    </script>
  </table>
</body>

</html>
