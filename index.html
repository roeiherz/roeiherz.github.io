<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Roei Herzig</title>
  
  <meta name="author" content="Roei Herzig">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="4VZM_QHD0ktSdSTOi7khsldecOhYVsbJ7TPQkx5lvUc" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roei Herzig</name>
              </p>
              <p>
                Hi there! I'm Roei, a second-year CS Ph.D. student at <a href="https://www.tau.ac.il/">Tel Aviv University</a>,
<!--                working with <a href="https://research.nvidia.com/person/gal-chechik">Prof. Gal Chechik</a>,-->
                working with  <a href="http://www.cs.tau.ac.il/~gamir/">Prof. Amir Globerson </a>
                and <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>,
                and a visiting student at <a href="https://bair.berkeley.edu/">the Berkeley AI Research Lab</a>.
              </p>
              <p>
                I'm also a Machine Learning & Deep Learning Researcher, I have worked at <a href="https://www.getnexar.com/"> Nexar </a> and
                <a href="https://traxretail.com/"> Trax Image Recognition </a> in the last 5 years.
                Previously, I graduated <i> magna cum laude</i> from <a href="https://www.tau.ac.il/">Tel Aviv University</a>
                with MSc (CS), BSc (CS) and BSc (Physics).
              </p>
              <p>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                I'm looking for a strong MSc students that wish to collaborate and publish in top-tier conferences on
                <em>Geometry and Learning for 3D</em> and <em>Video Understanding</em>.
              </p>
              <p style="text-align:center">
                <a href="mailto:roeiherz@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/roeiherzig">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/roeiherz">Github</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp-->
<!--                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp-->
                <a href="https://www.linkedin.com/in/roei-herzig-7534615a/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://scholar.google.co.il/citations?user=6Q-289IAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/RoeiHerzig.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/RoeiHerzig_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--        Research-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I mainly work on <b>structured models</b> in machine learning and deep learning
                for obtaining a better <b>semantic understanding in videos and images</b> (e.g., Structured Prediction).
                I believe our world is compositional, and humans do not perceive the world as raw pixels.
                Moreover, structured models can enjoy generalization and inductive-bias properties, which I find critical,
                mostly at the intersections of vision, language, and robotics.

<!--                I mainly focus on machine learning models and deep learning methods for <b>structured semantic understanding in videos and images</b>-->
<!--                (e.g. Structured Prediction). I believe our world is compositional and humans don't perceive the world as raw pixels.-->
<!--                Moreover, structured models can enjoy the properties of generalization and inductive-bias, which I find critical, especially at the intersections of vision, language and robotics.-->
              </p>
              <p>
                Research Interest: <br>
                <ul>
                  <li><i>Machine Learning & Deep Learning</i>: Generative Models, Graph Neural Networks, Self-Supervised Learning. </li>
                  <li><i>Vision & Language</i>: Video Understanding, Scene Understanding, Visual Reasoning. </li>
                  <li><i>Vision & Robotics</i>: Semantic Understanding, Structured Representation, Transfer Learning. </li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>

<!--        Personal-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Personal</heading>
              <p>
                I'm a proud father of Adam, and when I'm not working,
                I'm also a history buff and love learning about science, politics, the two World Wars, the Cold War, and music.
              </p>
            </td>
          </tr>
        </tbody></table>

<!--        Publications-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ag2vid.gif" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2006.15327">
                  <papertitle>Compositional Video Synthesis with Action Graphs</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                <a href="http://www.amirbar.net/">Amir Bar*</a>,
                <strong>Roei Herzig*</strong>,
                <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
                <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
                <br>
  <!--              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020-->
                <em> ArXiv preprint </em>, 2020
                <br>
                <a href="https://roeiherz.github.io/AG2Video/">project page</a> /
                <a href="https://github.com/roeiherz/AG2Video">code</a> /
                <a href="https://www.dropbox.com/s/yh110evfjw3bwkf/ActionGraph.pdf?dl=0">slides</a> /
                <a href="data/2020ActionGraphs.txt">bibtex</a>
                <br>
                <p></p>
                <p>
                  We introduce the formalism of <em>Action Graphs</em>,
                  a natural and convenient structure representing the dynamics of actions between objects over time.
                  We show we can synthesize goal-oriented videos on the CATER and Something Something datasets
                  and generate novel compositions of unseen actions.
                </p>
              </td>
        </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CanonicalSg2Im.gif" alt="fast-texture" width="200" height="100">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.07414">
                <papertitle>Learning Canonical Representations for Scene Graph to Image Generation</papertitle>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              </a>
              <br>
              <strong>Roei Herzig*</strong>,
              <a href="http://www.amirbar.net/">Amir Bar*</a>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
              <em> Proceedings of the European Conference on Computer Vision (ECCV) </em>, 2020
              <br>
              <a href="https://roeiherz.github.io/CanonicalSg2Im/">project page</a> /
              <a href="https://github.com/roeiherz/CanonicalSg2Im">code</a> /
              <a href="data/herzig2019canonical.txt">bibtex</a>
              <br>
              <p></p>
              <p>
                  We present a novel model that can inherently learn canonical graph representations and show better
                  robustness to graph size, adversarial attacks, and semantic equivalent,
                  thus generating superior images of complex visual scenes.
              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stin.png" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.09930">
                <papertitle>Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</papertitle>
              </a>
              <br>
              Joanna Materzynska,
              <a href="http://tetexiao.com/">Tete Xiao</a>,
              <strong>Roei Herzig</strong>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang*</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a>
              <br>
              <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2020
              <br>
              <a href="https://joaanna.github.io/something_else/">project page</a> /
              <a href="https://github.com/joaanna/something_else">code</a> /
              <a href="https://github.com/joaanna/something_else">dataset</a> /
              <a href="data/CVPR2020_SomethingElse.bib">bibtex</a>
              <br>
              <p></p>
              <p>

                We propose a novel compositional action recognition task where the training combinations of verbs and nouns do not overlap with the test set.
                We show the effectiveness of our approach on the proposed compositional task and
                a few-shot compositional setting which requires the model to generalize across both object appearance and action category.

              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dsg.jpg" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.10200">
                <papertitle>Differentiable Scene Graphs</papertitle>
              </a>
              <br>
              Moshiko Raboh* ,
              <strong>Roei Herzig*</strong>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020
              <br>
              <a href="https://github.com/shikorab/DSG">code</a> /
              <a href="data/herzig2019DSG.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose an intermediate “graph-like” representation (<i>DSGs</i>) that can be learned in an end-to-end manner
                from the supervision for a downstream visual reasoning task, which achieves a new state-of-the-art results
                on <i>Referring Relationships</i> task.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scene_understanding_resize.gif" alt="fast-texture" width="200" height="160", loop=infinite>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.01233">
                <papertitle>Spatio-Temporal Action Graph Networks</papertitle>
<!--                <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Elad Levi* ,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://people.eecs.berkeley.edu/~hangg/">Hang Gao</a>,
              Eli Brosh,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
<!--              <em> IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) </em>, 2019&nbsp <font color="red"><strong>(Oral)</strong></font>-->
              <em> Workshop on Autonomous Driving at ICCV </em>, 2019&nbsp<font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/roeiherz/STAG-Nets">code</a> /
              <a href="data/herzig2019STAG.bib">bibtex</a>
              <br>
              <p></p>
              <p> We propose a <i>latent</i> inter-object graph representation for activity recognition
                that explores the visual interaction between the objects in a self-supervised manner. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/localizationNexar.jpg" alt="fast-texture" width="200" height="160", loop=infinite>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1905.03706">
                <papertitle>Accurate Visual Localization for Automotive Applications</papertitle>
              </a>
              <br> Eli Brosh*, Matan Friedmann*, Ilan Kadar*, Lev Yitzhak Lavy*, Elad Levi*, Shmuel Rippa*,
              Yair Lempert, Bruno Fernandez-Ruiz, <strong>Roei Herzig</strong>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
              <em> Workshop on Autonomous Driving at CVPR </em>, 2019
              <br>
              <a href="https://blog.getnexar.com/announcing-ai-powered-image-retrieval-method-for-better-localization-in-cities-6fc482b4b0f">blog</a> /
              <a href="https://github.com/getnexar/Nexar-Visual-Localization">code</a> /
              <a href="https://github.com/getnexar/Nexar-Visual-Localization#dataset--benchmark">dataset</a> /
              <a href="data/herzig2019localization.bib">bibtex</a>
              <br>
              <p></p>
              <p> We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues with on
                a new large-scale driving dataset based on video and GPS data.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/densely_packed_scenes_store.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Goldman_Precise_Detection_in_Densely_Packed_Scenes_CVPR_2019_paper.html">
                <papertitle>Precise Detection in Densely Packed Scenes</papertitle>
              </a>
              <br>
              Eran Goldman*,
              <strong>Roei Herzig*</strong>, Aviv Eisenschtat* ,
              <a href="http://www.eng.biu.ac.il/goldbej/">Jacob Goldberger</a>,
              <a href="https://talhassner.github.io/home/">Tal Hassner</a>
              <br>
              <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2019
              <br>
              <a href="https://github.com/eg4000/SKU110K_CVPR19">code</a> /
              <a href="https://github.com/eg4000/SKU110K_CVPR19#dataset">dataset</a> /
              <a href="data/herzig2019dense.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We collect a new <i>SKU-110K</i> dataset which takes detection challenges to unexplored territories,
                and propose a novel mechanism to learn deep overlap rates for each detection. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mapping_imgs2sg.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction.pdf">
                <papertitle>Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</papertitle>
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Moshiko Raboh* ,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson </a>
              <br>
              <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2018
              <br>
              <a href="https://github.com/shikorab/SceneGraph">code</a> /
              <a href="data/herzig2018img2sg.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose a novel invariant graph network for mapping images to scene graphs using the permutation invariant
                property, which achieves a new state-of-the-art results on Visual Genome dataset. </p>
            </td>
          </tr>
        </tbody></table>


  <!--        Talks-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Invited Talks</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://youtu.be/9HOFC4ffOuY?t=8543">
                  <papertitle>Learning Canonical Representations for Scene Graph to Image Generation</papertitle>
                </a>
                <a href="https://docs.google.com/document/d/1doRVWC27mn4d_Jqbn-zRONn72WpmkObG1qf85WNzxuU/edit">(BAIR Fall Seminar, 2020)</a>,
                <a href="https://www.dropbox.com/s/41gnguteyaaaekd/R.%20Herzig_13.pdf?dl=0">Slides</a>.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://youtu.be/c8_32IVn-sg?t=12965">
                  <papertitle>Compositional Video Synthesis with Action Graphs</papertitle>
                </a>
                <a href="https://gdl-israel.github.io/index.html">(Israeli Geometric Deep Learning, 2020)</a>,
                <a href="https://www.dropbox.com/s/8vflpwvm1kekgyn/AG2Vid.pdf?dl=0">Slides</a>.
                <br>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Structured Semantic Understanding for Videos and Images</papertitle>
                (Advanced Seminar in Computer Graphics at TAU, 2020),
                <a href="https://www.dropbox.com/s/sjqfk6obovb2295/Research_160620.pdf?dl=0">Slides</a>.
                <br>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://github.com/jonbarron/jonbarron_website">Webside template credits </a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143788442-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-143788442-1');
    </script>
  </table>
</body>

</html>
