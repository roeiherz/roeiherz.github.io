<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Roei Herzig</title>

  <meta name="author" content="Roei Herzig">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-site-verification" content="4VZM_QHD0ktSdSTOi7khsldecOhYVsbJ7TPQkx5lvUc" />

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Roei Herzig</name>
              </p>
                <p>
                Hi there! I am Roei, a Postdoctoral Scholar in the <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab</a> (BAIR) at UC Berkeley,
                working with <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>.
                    Additionally, I recently worked under the supervision of <a href="https://cs3801.wixsite.com/amirgloberson">Prof. Amir Globerson </a> at <a href="https://www.tau.ac.il/">Tel Aviv University</a> between April 2019 and April 2023.
                    I'm also affiliated as a research scientist at <a href="https://research.ibm.com/artificial-intelligence">IBM Research AI</a>.
              </p>
<!--                Additionally, I recently completed my Ph.D. degree in computer science at <a href="https://www.tau.ac.il/">Tel Aviv University</a> under the supervision of <a href="https://cs3801.wixsite.com/amirgloberson">Prof. Amir Globerson </a> between April 2019 and April 2023.-->
<!--                If you're around the Bay Area, come say hello!-->
<!--              <p>-->
<!--                Hi there! I am Roei, a graduated (04/2019-04/2023) CS Ph.D. student at <a href="https://www.tau.ac.il/">Tel Aviv University</a>-->
<!--                and a visiting scholar in <a href="https://bair.berkeley.edu/">Berkeley AI Research Lab</a> (BAIR),-->
<!--                working with  <a href="https://cs3801.wixsite.com/amirgloberson">Prof. Amir Globerson </a>-->
<!--                and <a href="https://people.eecs.berkeley.edu/~trevor/">Prof. Trevor Darrell</a>.-->
<!--                I'm also affiliated as a research scientist at <a href="https://research.ibm.com/artificial-intelligence">IBM Research AI</a>.-->
<!--              </p>-->
              <p>
                Previously, I graduated <i> magna cum laude</i> from <a href="https://www.tau.ac.il/">Tel Aviv University</a>
                with MSc (CS), BSc (CS), and BSc (Physics), and worked as a Machine Learning & Deep Learning researcher
                at <a href="https://www.getnexar.com/"> Nexar </a> and <a href="https://traxretail.com/"> Trax Image Recognition </a> for 5 years.
              </p>

<!--              <p>-->

<!--                  My research is generously supported by the Israeli Higher Education Council for Excellent Data Science Postdoctoral Scholars fellowship.-->
<!--              </p>-->

              <p>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                  I was awarded the 2023 IAAI Best PhD thesis <a href="https://iaai.org.il/prizes/">award</a> for the outstanding thesis in the field of Artificial Intelligence in Israel.
              </p>


                <p>
                <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                I'm looking for strong Master's and senior undergrads to collaborate and publish in top-tier conferences
                    on <em>Vision & Language</em>, <em> Vision & Robotics</em>, and <em>Video Understanding</em>.
              </p>
              <p style="text-align:center">
                <a href="mailto:roeiherz@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://twitter.com/roeiherzig">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/roeiherz">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/roei-herzig-7534615a/"> LinkedIn </a> &nbsp/&nbsp
                <a href="https://www.dropbox.com/scl/fi/xkykhat4ht4dc7rd63amm/Roi_Herzig_CV_updated.pdf?rlkey=nm9xmfaqslilrvv6kxqh9nttq&dl=0">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.il/citations?user=6Q-289IAAAAJ&hl=en">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
<!--              <a href="images/RoeiHerzig.jpg"><img style="width:70%;max-width:100%" alt="profile photo" src="images/RoeiHerzig_circle.jpg" class="hoverZoomLink"></a>-->
                <a href="images/roei_IBM_potrait_resized_small.png"><img style="width:70%;max-width:100%" alt="profile photo" src="images/roei_IBM_potrait_resized_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--        Research-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>

            Compositionality is a fundamental aspect of human cognition and language understanding.
                  At its core, it is the ability to decompose complex entities or concepts into simpler,
                  more basic components, and then combine these components in various ways in order to create more complex entities.
                  Despite their impressive capabilities, AI models are limited in this area,
                  and they have difficulty solving tasks that require generalization beyond the training distribution.
                  My research goal is to <b>develop compositionality</b> into intelligent machines in order to
                  improve robustness and generalization across a wide range of fields, such as vision, language, and robotics.


<!--                My research goal is to <b>develop compositionality</b> into intelligent machines to improve robustness and generalization-->
<!--                in multiple domains, such as vision, language, and robotics.-->
<!--                I believe our understanding of the world is naturally hierarchical and structured,-->
<!--                and intelligent machines would need to develop a compositional understanding that is robust and generalizable.-->
<!--                However, many existing vision architectures are not compositional,-->
<!--                and thus my research goal is to design compositional models that leverage inductive biases into-->
<!--                our architectures to generalize well across various tasks.-->
              </p>
<!--              <p>-->
<!--                Research Interest: <br>-->
<!--                <ul>-->
<!--                  <li><i>Machine Learning & Deep Learning</i>: Generative Models, Graph Neural Networks, Self-Supervised Learning. </li>-->
<!--                  <li><i>Vision & Language</i>: Video Understanding, Scene Understanding, Visual Reasoning. </li>-->
<!--                  <li><i>Vision & Robotics</i>: Semantic Understanding, Object-Centric Representation, Structured Representation. </li>-->
<!--                </ul>-->
<!--              </p>-->
            </td>
          </tr>
        </tbody></table>

<!--        Personal-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Personal</heading>
              <p>
                I'm a proud father of Adam and Liam and happily married to <a href="https://www.instagram.com/esti.herzig/">Esti</a>,
                  my amazing wife and my partner for life.
                When I'm not working, I'm also a history buff and love learning about science, politics, music, and the two World Wars.

                <!--                I'm a proud father of Adam and Liam, and when I'm not working,-->
<!--                I'm also a history buff and love learning about science, politics, the two World Wars, the Cold War, and music.-->
              </p>
            </td>
          </tr>
        </tbody></table>

<!--&lt;!&ndash;        News&ndash;&gt;-->
<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:100%;vertical-align:middle">-->
<!--              <heading>News</heading>-->
<!--            </td>-->
<!--          </tr>-->
<!--        </tbody></table>-->
<!--        <ul>-->
<!--          <li>I'm part of the organizing committee of <a href="https://nips.cc/Conferences/2023/Committees">NeurIPS 2023</a>.</li>-->
<!--          <li>I'm currently a part-time student researcher at FAIR, working with <a href="http://yann.lecun.com/">Yann LeCun</a>.</li>-->
<!--        </ul>-->

<!--        Preprints-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/llarva.gif" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.11815">
                  <papertitle>LLARVA: Vision-Action Instruction Tuning Enhances Robot Learning</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                  Dantong Niu*,
                  Yuvan Sharma*, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>&dagger;,
                  <strong>Roei Herzig</strong>&dagger;
                <br>
                  <em> Technical Report, 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
                <a href="https://llarva24.github.io/">project page</a> /
                <a href="https://github.com/Dantong88/LLARVA">code</a> /
                <a href="data/niu2024llarva.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We propose LLARVA, a model trained with a novel instruction tuning method that
                    leverages structured prompts to unify a range of robotic configurations
                    and introduces the concept of visual traces to align the vision and action spaces further.

                </p>
              </td>

        </tr>


        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/traveler.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2404.01476">
                  <papertitle>TraveLER: A Multi-LMM Agent Framework for Video Question-Answering</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                  Chuyi Shang*,
                  Amos You*,
                  <a href="https://people.eecs.berkeley.edu/~sanjayss/">Sanjay Subramanian</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                  <strong>Roei Herzig</strong>,
                <br>
                  <em> Technical Report, 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://alonmendelson.github.io/SGVL/">project page</a> /-->
<!--                <a href="https://github.com/AlonMendelson/SGVL">code</a> /-->
<!--                <a href="data/herzig2023SGVL.bib">bibtex</a>-->
                <br>
                <p></p>
                <p>
                    We present TraveLER, a modular multi-LMM agent framework for video question-answering
                    that does not require task-specific fine-tuning or annotations.
                    Through interactive question-asking using several agents with different roles,
                    our framework aims to answer the question by collecting relevant information from keyframes.
                </p>
              </td>

        </tr>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/RVP.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.02249">
                  <papertitle>Recursive Visual Programming</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                  Jiaxin Ge,
                  <a href="https://people.eecs.berkeley.edu/~sanjayss/">Sanjay Subramanian</a>,
                  <a href="https://bfshi.github.io/">Baifeng Shi</a>,
                  <strong>Roei Herzig</strong>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
                <br>
                  <em> Technical Report, 2024 </em>
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://alonmendelson.github.io/SGVL/">project page</a> /-->
<!--                <a href="https://github.com/AlonMendelson/SGVL">code</a> /-->
<!--                <a href="data/herzig2023SGVL.bib">bibtex</a>-->
                <br>
                <p></p>
                <p>
                    We present Recursive Visual Programming (RVP), a new approach of visual programming that simplifies generated routines,
                    allows more efficient problem solving, and manages more complex data structures.
                </p>
              </td>

        </tr>


<!--        Publications-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/CCoT.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2311.17076">
                  <papertitle>Compositional Chain-of-Thought Prompting for Large Multimodal Models</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                  Chancharik Mitra,
                  Brandon Huang,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <strong>Roei Herzig</strong>
                <br>
                  <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2024
                 <br>
<!--                <a href="https://alonmendelson.github.io/SGVL/">project page</a> /-->
                <a href="https://github.com/chancharikmitra/CCoT">code</a> /
                <a href="data/mitra2014CCOT.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We propose Compositional Chain-of-Thought (CCoT), a novel zero-shot Chain-of-Thought prompting
                    method that utilizes scene graph representations in order to extract compositional knowledge from an LMM.
                </p>
              </td>

        </tr>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/U2seg.jpg" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2312.17243">
                  <papertitle>U2Seg: Unsupervised Universal Image Segmentation</papertitle>
                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">
                </a>
                <br>
                  Dantong Niu*,
                  <a href="https://people.eecs.berkeley.edu/~xdwang/">Xudong Wang*</a>,
                  Xinyang Han*,
                  Long Lian,
                  <strong>Roei Herzig</strong>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
                <br>
                  <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2024
<!--                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023-->
                 <br>
<!--                <a href="https://github.com/u2seg/U2Seg?tab=readme-ov-file">project page</a> /-->
                <a href="https://github.com/u2seg/U2Seg">code</a> /
                <a href="data/niu2023unsupervised.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We present U2Seg, a unified framework for Unsupervised Universal image Segmentation
                    that consistently outperforms previous state-of-the-art methods.
                </p>
              </td>

        </tr>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SGVL.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2305.06343">
                  <papertitle>Incorporating Structured Representations into Pretrained Vision & Language Models Using Scene Graphs</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                <strong>Roei Herzig*</strong>,
                  Alon Mendelson*,
                <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
                  Assaf Arbelle,
                  Rogerio Feris,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
                <em> Conference on Empirical Methods in Natural Language Processing (EMNLP) </em> , 2023
<!--                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022-->
<!--                  <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2022-->
                 <br>
                <a href="https://alonmendelson.github.io/SGVL/">project page</a> /
                <a href="https://github.com/AlonMendelson/SGVL">code</a> /
                <a href="data/herzig2023SGVL.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We propose to improve pretrained VLMs, which are usually trained on large-scale image-text pairs,
                    by designing a specialized model architecture and a new training method that utilizes
                    a small set of scene graph annotations from the Visual Genome dataset
                    that are richer and reflect structured visual and textual information.

                </p>
              </td>

        </tr>

        <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/DAC.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2305.19595">
                  <papertitle>Dense and Aligned Captions Promote Compositional Reasoning in VL Models</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                  Sivan Doveh,
                  Assaf Arbelle,
                  Sivan Harary,
                  <strong>Roei Herzig</strong>,
                  Donghyun Kim,
                  Paola Cascante-bonilla,
                  Amit Alfassy,
                  Rameswar Panda,
                  Raja Giryes,
                  Rogerio Feris,
                  Shimon Ullman,
                Leonid Karlinsky
                  <br>
<!--                <em> Tech report </em> , 2023-->
<!--                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022-->
                  <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2023 &nbsp<font color="red"><strong>(Spotlight)</strong></font>
                 <br>
                <a href="https://github.com/SivanDoveh/DAC">project page</a> /
                <a href="https://github.com/SivanDoveh/DAC">code</a> /
                <a href="data/herzig2023SGVL.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                   We propose a fine-tuning approach for automatically treating two factors limiting VL models’ compositional reasoning performance:
                    (i) the caption quality, or in other words 'image alignment', of the texts;
                    and (ii) the level of caption density, which refers to the number of details that appear in the image.

                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/PromptonomyViT.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2212.04821">
                  <papertitle>PromptonomyViT: Multi-Task Prompt Learning Improves Video Transformers using Synthetic Scene Data</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                <strong>Roei Herzig*</strong>,
                  Ofir Abramovich*,
                  Elad Ben-Avraham,
                  Assaf Arbelle,
                <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
                  <a href="https://faculty.runi.ac.il/arik/site/index.asp">Ariel Shamir</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
                <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2024
                  <br>
                <a href="https://ofir1080.github.io/PromptonomyViT/">project page</a> /
                <a href="https://github.com/ofir1080/PromptonomyViT">code</a> /
                <a href="data/herzig2022PromptonomyViT.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We present <em>PromptonomyViT</em>, a model that leverages a multi-task prompt learning
                    approach for video transformers, where a shared transformer backbone
                    is enhanced with task-specific prompts.

                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SVLC.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2211.11733">
                  <papertitle>Teaching Structured Vision & Language Concepts to Vision & Language Models</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                  Sivan Doveh,
                  Assaf Arbelle,
                  Sivan Harary,
                  Rameswar Panda,
                  <strong>Roei Herzig</strong>,
                  Eli Schwartz,
                  Donghyun Kim,
                  Raja Giryes,
                  Rogerio Feris,
                  Shimon Ullman,
                Leonid Karlinsky
                  <br>
                <a href="https://github.com/SivanDoveh/TSVLC">code</a> /
                <a href="data/herzig2023SVLC.bib">bibtex</a>
                <br>
<!--                <em> Tech report </em> , 2022-->
                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2023
<!--                  <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2022-->
                 <br>
                <p></p>
                <p>
                    We demonstrate language augmentation techniques for teaching language
                    structure to VL models.

                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/SViT.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2206.06346">
                  <papertitle>Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                Elad Ben-Avraham,
                <strong>Roei Herzig</strong>,
                <a href="https://karttikeya.github.io/">Karttikeya Mangalam</a>,
                <a href="http://www.amirbar.net/">Amir Bar</a>,
                <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>,
                <br>
                <a href="https://scholar.google.com/citations?user=WbO7tjYAAAAJ&hl=en">Leonid Karlinsky</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
<!--                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022-->
                  <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2022
                  <br>
                  <span style="color: goldenrod;">
<!--                  <span style="color: green;">-->
                  <em> Winner of the Ego4D CVPR'22 Point of No Return Temporal Localization Challenge</em> , 2022
                  </span>
                 <br>
                <a href="https://eladb3.github.io/SViT/">project page</a> /
                <a href="https://github.com/eladb3/SViT">code</a> /
                <a href="data/eladherz2022svit.bib">bibtex</a>
                <br>
                <p></p>
                <p>
                    We present <em>SViT</em> (for <i>Structured Video Tokens</i>), a model that utilizes the structure
                    of a small set of <em>images</em>, whether they are within or outside the domain of interest,
                    available only during training for a <em>video</em> downstream task.

                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/FETA.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2209.03648">
                  <papertitle>FETA: Towards Specializing Foundational Models for Expert Task Applications</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                  Amit Alfassy,
                  Assaf Arbelle,
                  Oshri Halimi,
                  Sivan Harary,
                <strong>Roei Herzig</strong>,
                Eli Schwartz,
                Rameswar Panda,
                Michele Dolfi,
                Christoph Auer,
                  Kate Saenko,
                  Peter Staar,
                  Rogerio Feris,
                  Leonid Karlinsky
                <br>
                  <em> NeurIPS Datasets and Benchmarks </em> , 2022
                <p></p>
                <p>
                    We present <em>FETA</em>, a novel benchmark for evaluating and improving
                    Foundation Vision and Language Models performance on expert data tasks,
                    such as technical document understanding.
                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ORViT.gif" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2110.06915">
                  <papertitle>Object-Region Video Transformers</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                <strong>Roei Herzig</strong>,
                Elad Ben-Avraham,
                <a href="https://karttikeya.github.io/">Karttikeya Mangalam</a>,
                <a href="http://www.amirbar.net/">Amir Bar</a>,
                <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
                <br>
                <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022
                <br>
                <a href="https://roeiherz.github.io/ORViT/">project page</a> /
                <a href="https://github.com/roeiherz/ORViT">code</a> /
<!--                <a href="https://www.dropbox.com/s/yh110evfjw3bwkf/ActionGraph.pdf?dl=0">slides</a> /-->
                <a href="ORViT/data/2021ORViT.txt">bibtex</a>
                <br>
                <p></p>
                <p>
                  We present <em>ORViT</em>, an object-centric approach that extends video transformer layers with a block that
                  directly incorporates object representations.

                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="BrAD/BrAD_teaser.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.02300">
                  <papertitle>Unsupervised Domain Generalization by Learning a Bridge Across Domains</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                Sivan Harary*, Eli Schwartz*, Assaf Arbelle*, Peter Staar, Shady Abu-Hussein,
                <br>
                Elad Amrani,
                <strong>Roei Herzig</strong>,
                Amit Alfassy, Raja Giryes, Hilde Kuehne, Dina Katabi,
                <br>
                Kate Saenko, Rogerio Feris, Leonid Karlinsky
                <br>
                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022
                <br>
<!--                <a href="https://roeiherz.github.io/ORViT/">project page</a> /-->
<!--                <a href="https://github.com/roeiherz/ORViT">code</a> /-->
<!--                <a href="https://www.dropbox.com/s/yh110evfjw3bwkf/ActionGraph.pdf?dl=0">slides</a> /-->
<!--                <a href="data/2020ActionGraphs.txt">bibtex</a>-->
<!--                <br>-->
                <p></p>
                <p>
                  We present a novel self-supervised cross-domain learning method based on semantically aligning
                  all the domains to a common <em>BrAD</em> domain - a learned auxiliary bridge domain as an edge map with image-to-image mappings.
                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="DETReg/detreg_after.png" alt="fast-texture" width="200" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2106.04550">
                  <papertitle>DETReg: Unsupervised Pretraining with Region Priors for Object Detection</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                <a href="http://www.amirbar.net/">Amir Bar</a>,
                <a href="https://xinw.ai/">Xin Wang</a>,
                Vadim Kantorov,
                <a href="https://people.eecs.berkeley.edu/~cjrd/">Colorado J Reed</a>,
                <strong>Roei Herzig</strong>,
                <br>
                <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
                <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
                <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2022
                <br>
                <a href="https://www.amirbar.net/detreg/">project page</a> /
                <a href="https://github.com/amirbar/detreg">code</a> /
                <a href="https://youtu.be/1rxz_SWB7gQ?t=443">Video</a>
                <br>
                <p></p>
                <p>
                  Pretraining transformers to localize potential objects improves object detection.
                </p>
              </td>
        </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/ag2vid.gif" alt="fast-texture" width="200" height="140">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2006.15327">
                  <papertitle>Compositional Video Synthesis with Action Graphs</papertitle>
<!--                  <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
                </a>
                <br>
                <a href="http://www.amirbar.net/">Amir Bar*</a>,
                <strong>Roei Herzig*</strong>,
                <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
                <a href="https://anna-rohrbach.net/">Anna Rohrbach</a>,
                <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
                <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
                <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
                <br>
                <em>  International Conference on Machine Learning (ICML) </em>, 2021
                <br>
                <a href="https://roeiherz.github.io/AG2Video/">project page</a> /
                <a href="https://github.com/roeiherz/AG2Video">code</a> /
                <a href="https://www.dropbox.com/s/yh110evfjw3bwkf/ActionGraph.pdf?dl=0">slides</a> /
                <a href="data/2020ActionGraphs.txt">bibtex</a>
                <br>
                <p></p>
                <p>
                  We introduce the formalism of <em>Action Graphs</em>,
                  a natural and convenient structure representing the dynamics of actions between objects over time.
                  We show we can synthesize goal-oriented videos on the CATER and Something Something datasets
                  and generate novel compositions of unseen actions.
                </p>
              </td>
        </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CanonicalSg2Im.gif" alt="fast-texture" width="200" height="100">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.07414">
                <papertitle>Learning Canonical Representations for Scene Graph to Image Generation</papertitle>
<!--                <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
              </a>
              <br>
              <strong>Roei Herzig*</strong>,
              <a href="http://www.amirbar.net/">Amir Bar*</a>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu</a>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>,
              <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
              <br>
              <em> Proceedings of the European Conference on Computer Vision (ECCV) </em>, 2020
              <br>
              <a href="https://roeiherz.github.io/CanonicalSg2Im/">project page</a> /
              <a href="https://github.com/roeiherz/CanonicalSg2Im">code</a> /
              <a href="data/herzig2019canonical.txt">bibtex</a>
              <br>
              <p></p>
              <p>
                  We present a novel model that can inherently learn canonical graph representations and show better
                  robustness to graph size, adversarial attacks, and semantic equivalent,
                  thus generating superior images of complex visual scenes.
              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/stin.png" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1912.09930">
                <papertitle>Something-Else: Compositional Action Recognition with Spatial-Temporal Interaction Networks</papertitle>
              </a>
              <br>
              Joanna Materzynska,
              <a href="http://tetexiao.com/">Tete Xiao</a>,
              <strong>Roei Herzig</strong>,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang*</a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell*</a>
              <br>
              <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2020
              <br>
              <a href="https://joaanna.github.io/something_else/">project page</a> /
              <a href="https://github.com/joaanna/something_else">code</a> /
              <a href="https://github.com/joaanna/something_else">dataset</a> /
              <a href="data/CVPR2020_SomethingElse.bib">bibtex</a>
              <br>
              <p></p>
              <p>

                We propose a novel compositional action recognition task where the training combinations of verbs and nouns do not overlap with the test set.
                We show the effectiveness of our approach on the proposed compositional task and
                a few-shot compositional setting which requires the model to generalize across both object appearance and action category.

              </p>
            </td>
      </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dsg.jpg" alt="fast-texture" width="200" height="140">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1902.10200">
                <papertitle>Differentiable Scene Graphs</papertitle>
              </a>
              <br>
              Moshiko Raboh* ,
              <strong>Roei Herzig*</strong>,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
              <br>
              <em> Winter Conference on Applications of Computer Vision (WACV) </em>, 2020
              <br>
              <a href="https://github.com/shikorab/DSG">code</a> /
              <a href="data/herzig2019DSG.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose an intermediate “graph-like” representation (<i>DSGs</i>) that can be learned in an end-to-end manner
                from the supervision for a downstream visual reasoning task, which achieves a new state-of-the-art results
                on <i>Referring Relationships</i> task.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/scene_understanding_resize.gif" alt="fast-texture" width="200" height="160", loop=infinite>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1812.01233">
                <papertitle>Spatio-Temporal Action Graph Networks</papertitle>
<!--                <img src="images/new.gif" alt="fast-texture" width="25" height="11">-->
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Elad Levi* ,
              <a href="https://cs-people.bu.edu/hxu/">Huijuan Xu*</a>,
              <a href="https://people.eecs.berkeley.edu/~hangg/">Hang Gao</a>,
              Eli Brosh,
              <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>,
              <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>,
              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>
              <br>
<!--              <em> IEEE/CVF International Conference on Computer Vision Workshop (ICCVW) </em>, 2019&nbsp <font color="red"><strong>(Oral)</strong></font>-->
              <em> Workshop on Autonomous Driving at ICCV </em>, 2019&nbsp<font color="red"><strong>(Oral)</strong></font>
              <br>
              <a href="https://github.com/roeiherz/STAG-Nets">code</a> /
              <a href="data/herzig2019STAG.bib">bibtex</a>
              <br>
              <p></p>
              <p> We propose a <i>latent</i> inter-object graph representation for activity recognition
                that explores the visual interaction between the objects in a self-supervised manner. </p>
            </td>
          </tr>

<!--          <tr>-->
<!--            <td style="padding:20px;width:25%;vertical-align:middle">-->
<!--              <img src="images/localizationNexar.jpg" alt="fast-texture" width="200" height="160", loop=infinite>-->
<!--            </td>-->
<!--            <td style="padding:20px;width:75%;vertical-align:middle">-->
<!--              <a href="https://arxiv.org/abs/1905.03706">-->
<!--                <papertitle>Accurate Visual Localization for Automotive Applications</papertitle>-->
<!--              </a>-->
<!--              <br> Eli Brosh*, Matan Friedmann*, Ilan Kadar*, Lev Yitzhak Lavy*, Elad Levi*, Shmuel Rippa*,-->
<!--              Yair Lempert, Bruno Fernandez-Ruiz, <strong>Roei Herzig</strong>,-->
<!--              <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>-->
<!--              <br>-->
<!--              <em> Workshop on Autonomous Driving at CVPR </em>, 2019-->
<!--              <br>-->
<!--              <a href="https://blog.getnexar.com/announcing-ai-powered-image-retrieval-method-for-better-localization-in-cities-6fc482b4b0f">blog</a> /-->
<!--              <a href="https://github.com/getnexar/Nexar-Visual-Localization">code</a> /-->
<!--              <a href="https://github.com/getnexar/Nexar-Visual-Localization#dataset&#45;&#45;benchmark">dataset</a> /-->
<!--              <a href="data/herzig2019localization.bib">bibtex</a>-->
<!--              <br>-->
<!--              <p></p>-->
<!--              <p> We propose a hybrid coarse-to-fine approach that leverages visual and GPS location cues with on-->
<!--                a new large-scale driving dataset based on video and GPS data.</p>-->
<!--            </td>-->
<!--          </tr>-->

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/densely_packed_scenes_store.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Goldman_Precise_Detection_in_Densely_Packed_Scenes_CVPR_2019_paper.html">
                <papertitle>Precise Detection in Densely Packed Scenes</papertitle>
              </a>
              <br>
              Eran Goldman*,
              <strong>Roei Herzig*</strong>, Aviv Eisenschtat* ,
              <a href="http://www.eng.biu.ac.il/goldbej/">Jacob Goldberger</a>,
              <a href="https://talhassner.github.io/home/">Tal Hassner</a>
              <br>
              <em> IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) </em> , 2019
              <br>
              <a href="https://github.com/eg4000/SKU110K_CVPR19">code</a> /
              <a href="https://github.com/eg4000/SKU110K_CVPR19#dataset">dataset</a> /
              <a href="data/herzig2019dense.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We collect a new <i>SKU-110K</i> dataset which takes detection challenges to unexplored territories,
                and propose a novel mechanism to learn deep overlap rates for each detection. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mapping_imgs2sg.jpg" alt="fast-texture" width="200" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://papers.nips.cc/paper/7951-mapping-images-to-scene-graphs-with-permutation-invariant-structured-prediction.pdf">
                <papertitle>Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</papertitle>
              </a>
              <br>
              <strong>Roei Herzig*</strong>, Moshiko Raboh* ,
              <a href="https://research.nvidia.com/person/gal-chechik">Gal Chechik</a>,
              <a href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a>,
              <a href="https://cs3801.wixsite.com/amirgloberson">Amir Globerson </a>
              <br>
              <em> Advanced in Neural Information Processing Systems (NeurIPS) </em> , 2018
              <br>
              <a href="https://github.com/shikorab/SceneGraph">code</a> /
              <a href="data/herzig2018img2sg.bib">bibtex</a>
              <br>
              <p></p>
              <p>
                We propose a novel invariant graph network for mapping images to scene graphs using the permutation invariant
                property, which achieves a new state-of-the-art results on Visual Genome dataset. </p>
            </td>
          </tr>
        </tbody></table>


  <!--        Talks-->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Talks</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>

            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href=https://youtu.be/wR6-hnrhjLo?t=15101">
                  <papertitle>Towards Compositionality in Large Multimodal Models</papertitle>
                </a>
                <a href="https://docs.google.com/document/d/1u4jGE5NQX2Zg_TRQ5E2hi4Ax3LTzEcMFVfOJpTk9kX8/edit">(Fall 2023 BAIR Visual AI Workshop)</a>, December 2023.
                <br>
<!--              </td>-->
            </tr>
            <br>


            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href=https://www.youtube.com/watch?v=zzdaJxIE0ms">
                  <papertitle>Towards Compositionality in Video Understanding</papertitle>
                </a>
                <a href="https://sites.google.com/view/israel-vision-day-2022/home/program">(Israeli Vision Day)</a>, January 2023.
                <br>
<!--              </td>-->
            </tr>
            <br>

            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href=https://www.dropbox.com/s/b52to9ngiq4pq2h/NeurIPS%20Highlights%20December%202022.pdf?dl=0">
                  <papertitle>NeurIPS 2022 Highlights</papertitle>
                </a>
                <a href=>(TAU fundamental of AI, Tel-Aviv University)</a>, December 2022.
                <br>
<!--              </td>-->
            </tr>
            <br>

          <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href=https://www.dropbox.com/s/za4csyuqh2hxll8/Compositionality%20December%202022.pdf?dl=0">
                  <papertitle>Towards Compositionality in Video Understanding</papertitle>
                </a>
                <a href=>(Vision and AI Seminar, Weizmann Institute of Science)</a>, December 2022.
                <br>
<!--              </td>-->
            </tr>
            <br>

            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href=https://www.dropbox.com/s/j9635r8ky99fjod/IAAI%202022.pdf?dl=0">
                  <papertitle>Towards Compositionality in Video Understanding</papertitle>
                </a>
                <a href="https://iaai22.net.technion.ac.il/program/">(Israeli Association for Artificial Intelligence Conference 2022)</a>, June 2022.
                <br>
<!--              </td>-->
            </tr>
            <br>

          <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://drive.google.com/file/d/1qYLD8ty_0EHpkKDCHG2Z7i3X4cQrfZdU/view">
                  <papertitle>ORViT: Object-Region Video Transformers</papertitle>
                </a>
                <a href="https://youtu.be/NzXYIM-5Xmk?t=14817">(BAIR Visual Computing Workshop)</a>, March 2022.
                <br>
<!--              </td>-->
            </tr>
            <br>


            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://www.youtube.com/watch?v=b937AwRDFg4">
                  <papertitle>Towards Compositionality in Video Understanding</papertitle>
                </a>
                <a href="https://www.imvc.co.il/Portals/164/1420%20-%20Roei%20Herzig.pdf">(IMVC 2021)</a>, Oct 2021.
                <br>
<!--              </td>-->
            </tr>
            <br>
            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://www.dropbox.com/s/8fl0lsnzshm7n3w/SRVU_Recording_2560x1440.mp4?dl=0">
                  <papertitle>Towards Compositionality in Video Understanding by Prof. Trevor Darrell</papertitle>
                </a>
                <a href="https://sites.google.com/view/srvu-iccv21-workshop/home?authuser=0">(ICCV21 SRVU Workshop)</a>, Oct 2021.
                <br>
<!--              </td>-->
            </tr>
            <br>
            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://youtu.be/qNKliNntvBE?t=11495">
                  <papertitle>Compositional Video Synthesis with Action Graphs</papertitle>
                </a>
                <a href="https://sites.google.com/view/vision-day-2020/home/program">(Israel Vision Day)</a>, Dec 2020.
<!--                <a href="https://www.dropbox.com/s/o9ceslphatwnctn/AG2Vid.pdf?dl=0">Slides</a>.-->
                <br>
<!--              </td>-->
            </tr>
            <br>
            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://youtu.be/9HOFC4ffOuY?t=8543">
                  <papertitle>Learning Canonical Representations for Scene Graph to Image Generation</papertitle>
                </a>
                <a href="https://docs.google.com/document/d/1doRVWC27mn4d_Jqbn-zRONn72WpmkObG1qf85WNzxuU/edit">(BAIR Fall Seminar)</a>, Aug 2020.
<!--                <a href="https://www.dropbox.com/s/41gnguteyaaaekd/R.%20Herzig_13.pdf?dl=0">Slides</a>.-->
                <br>
<!--              </td>-->
            </tr>
            <br>
            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                <a href="https://youtu.be/c8_32IVn-sg?t=12965">
                  <papertitle>Compositional Video Synthesis with Action Graphs</papertitle>
                </a>
                <a href="https://gdl-israel.github.io/index.html">(Israeli Geometric Deep Learning)</a>, Aug 2020.
<!--                <a href="https://www.dropbox.com/s/8vflpwvm1kekgyn/AG2Vid.pdf?dl=0">Slides</a>.-->
                <br>
<!--              </td>-->
            </tr>
            <br>
            <tr>
<!--              <td style="padding:20px;width:75%;vertical-align:middle">-->
                  <papertitle>Structured Semantic Understanding for Videos and Images</papertitle>
                (Advanced Seminar in Computer Graphics at TAU), Jun 2020.
<!--                <a href="https://www.dropbox.com/s/sjqfk6obovb2295/Research_160620.pdf?dl=0">Slides</a>.-->
                <br>
<!--              </td>-->
            </tr>
          </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Template:
                <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>, Last update: 12/2023.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>


    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143788442-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-143788442-1');
    </script>
  </table>
</body>

</html>
